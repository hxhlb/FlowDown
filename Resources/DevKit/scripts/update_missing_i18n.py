#!/usr/bin/env python3
"""
Update missing i18n translations in Localizable.xcstrings.
This script adds missing English localizations and fixes 'new' state translations.
"""

import json
import sys
import os

NEW_STRINGS: dict[str, dict[str, str]] = {
    "Proactive Memory": {"zh-Hans": "ä¸»åŠ¨æä¾›è®°å¿†"},
    "When enabled, FlowDown will include stored memories in system prompts and Shortcuts inference even if memory tools are disabled.": {
        "zh-Hans": "å¼€å¯åï¼Œå³ä½¿æœªå¯ç”¨è®°å¿†å·¥å…·ï¼ŒFlowDown ä¹Ÿä¼šåœ¨ç³»ç»Ÿæç¤ºè¯ä¸å¿«æ·æŒ‡ä»¤æ¨ç†ä¸­æä¾›å·²å­˜å‚¨çš„è®°å¿†ã€‚"
    },
    "Proactive Memory Context": {"zh-Hans": "ä¸»åŠ¨æä¾›çš„è®°å¿†æ‘˜è¦"},
    "Choose how FlowDown proactively shares stored memories with the model during conversations and Shortcuts automations.": {
        "zh-Hans": "é€‰æ‹© FlowDown åœ¨å¯¹è¯ä¸å¿«æ·æŒ‡ä»¤è‡ªåŠ¨åŒ–ä¸­å‘æ¨¡å‹ä¸»åŠ¨æä¾›è®°å¿†çš„æ–¹å¼ã€‚"
    },
    "Off": {"zh-Hans": "å…³é—­"},
    "Past Day": {"zh-Hans": "1 å¤©å†…"},
    "Past Week": {"zh-Hans": "1 å‘¨å†…"},
    "Past Month": {"zh-Hans": "1 ä¸ªæœˆå†…"},
    "Past Year": {"zh-Hans": "1 å¹´å†…"},
    "Latest 15 Items": {"zh-Hans": "æœ€è¿‘ 15 é¡¹"},
    "Latest 30 Items": {"zh-Hans": "æœ€è¿‘ 30 é¡¹"},
    "All Memories": {"zh-Hans": "æ‰€æœ‰"},
    "Proactive memory sharing is disabled.": {"zh-Hans": "å·²å…³é—­ä¸»åŠ¨æä¾›è®°å¿†ã€‚"},
    "Memories saved within the past 24 hours.": {"zh-Hans": "åŒ…å«è¿‡å» 24 å°æ—¶å†…ä¿å­˜çš„è®°å¿†ã€‚"},
    "Memories saved within the past 7 days.": {"zh-Hans": "åŒ…å«è¿‡å» 7 å¤©å†…ä¿å­˜çš„è®°å¿†ã€‚"},
    "Memories saved within the past 30 days.": {"zh-Hans": "åŒ…å«è¿‡å» 30 å¤©å†…ä¿å­˜çš„è®°å¿†ã€‚"},
    "Memories saved within the past year.": {"zh-Hans": "åŒ…å«è¿‡å»ä¸€å¹´å†…ä¿å­˜çš„è®°å¿†ã€‚"},
    "The most recent 15 memories.": {"zh-Hans": "åŒ…å«æœ€è¿‘çš„ 15 æ¡è®°å¿†ã€‚"},
    "The most recent 30 memories.": {"zh-Hans": "åŒ…å«æœ€è¿‘çš„ 30 æ¡è®°å¿†ã€‚"},
    "All stored memories.": {"zh-Hans": "åŒ…å«æ‰€æœ‰å·²å­˜å‚¨çš„è®°å¿†ã€‚"},
    "Scope: %@": {"zh-Hans": "èŒƒå›´ï¼š%@"},
    "%d. [%@] %@": {"zh-Hans": "%d. [%@] %@"},
    "This summary is provided automatically according to the user's proactive memory setting, even when memory tools are disabled.": {
        "zh-Hans": "è¯¥æ‘˜è¦æ ¹æ®ç”¨æˆ·çš„ä¸»åŠ¨è®°å¿†è®¾ç½®è‡ªåŠ¨æä¾›ï¼Œå³ä½¿è®°å¿†å·¥å…·æœªå¯ç”¨ä¹Ÿä¼šé™„å¸¦ã€‚"
    },
    "A proactive memory summary has been provided above according to the user's setting. Treat it as reliable context and keep it updated through memory tools when necessary.": {
        "zh-Hans": "æ ¹æ®ç”¨æˆ·çš„è®¾ç½®ï¼Œä¸Šæ–¹å·²æä¾›ä¸»åŠ¨è®°å¿†æ‘˜è¦ã€‚è¯·å°†å…¶è§†ä¸ºå¯é çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶åœ¨éœ€è¦æ—¶é€šè¿‡è®°å¿†å·¥å…·ä¿æŒæ›´æ–°ã€‚"
    },
    "Save to Conversation": {"zh-Hans": "ä¿å­˜åˆ°å¯¹è¯"},
    "Enable Memory": {"zh-Hans": "å¯ç”¨è®°å¿†"},
    "Save response to conversation history": {"zh-Hans": "å°†å›å¤ä¿å­˜åˆ°å¯¹è¯è®°å½•"},
    "Enable memory tools during inference": {"zh-Hans": "æ¨ç†æ—¶å¯ç”¨è®°å¿†å·¥å…·"},
    "Attachment shared via Shortcut.": {"zh-Hans": "é€šè¿‡å¿«æ·æŒ‡ä»¤åˆ†äº«çš„é™„ä»¶ã€‚"},
    "Quick Reply %@": {"zh-Hans": "å¿«é€Ÿå›å¤ %@"},
    "Classify Content": {"zh-Hans": "åˆ†ç±»å†…å®¹"},
    "Use the model to classify content into one of the provided candidates. If the model cannot decide, the first candidate is returned.": {
        "zh-Hans": "ä½¿ç”¨æ¨¡å‹å°†å†…å®¹åˆ†ç±»åˆ°æä¾›çš„å€™é€‰é¡¹ä¹‹ä¸€ã€‚å¦‚æœæ¨¡å‹æ— æ³•å†³å®šï¼Œåˆ™è¿”å›ç¬¬ä¸€ä¸ªå€™é€‰é¡¹ã€‚"
    },
    "Prompt": {"zh-Hans": "æç¤º"},
    "Candidates": {"zh-Hans": "å€™é€‰é¡¹"},
    "What content should be classified?": {"zh-Hans": "éœ€è¦åˆ†ç±»çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"},
    "Provide the candidate labels.": {"zh-Hans": "è¯·æä¾›å€™é€‰æ ‡ç­¾ã€‚"},
    "Classify Content with Image": {"zh-Hans": "åˆ†ç±»å†…å®¹ï¼ˆå«å›¾åƒï¼‰"},
    "Use the model to classify content with the help of an accompanying image. If the model cannot decide, the first candidate is returned.": {
        "zh-Hans": "ä½¿ç”¨æ¨¡å‹ç»“åˆé™„å¸¦çš„å›¾åƒå¯¹å†…å®¹è¿›è¡Œåˆ†ç±»ã€‚å¦‚æœæ¨¡å‹æ— æ³•å†³å®šï¼Œåˆ™è¿”å›ç¬¬ä¸€ä¸ªå€™é€‰é¡¹ã€‚"
    },
    "Add any additional details for the classification.": {"zh-Hans": "è¯·è¡¥å……ä»»ä½•é¢å¤–çš„åˆ†ç±»ç»†èŠ‚ã€‚"},
    "Select an image to accompany the request.": {"zh-Hans": "è¯·é€‰æ‹©è¦éšè¯·æ±‚é™„ä¸Šçš„å›¾åƒã€‚"},
    "An image is provided with this request. Consider the visual details when selecting the candidate.": {
        "zh-Hans": "æ­¤è¯·æ±‚é™„å¸¦å›¾åƒã€‚é€‰æ‹©å€™é€‰é¡¹æ—¶è¯·å‚è€ƒè§†è§‰ç»†èŠ‚ã€‚"
    },
    "Classify + Image": {"zh-Hans": "åˆ†ç±» + å›¾åƒ"},
    "Classify ${image}": {"zh-Hans": "åˆ†ç±» ${image}"},
    "You are a classification assistant. Choose the best candidate for the provided content.": {
        "zh-Hans": "ä½ æ˜¯ä¸€ååˆ†ç±»åŠ©æ‰‹ã€‚è¯·ä¸ºæä¾›çš„å†…å®¹é€‰æ‹©æœ€åˆé€‚çš„å€™é€‰é¡¹ã€‚"
    },
    "Respond with exactly one candidate string from the list above. If you are unsure, respond with '%@'.": {
        "zh-Hans": "ä»ä¸Šè¿°åˆ—è¡¨ä¸­ä»…è¿”å›ä¸€ä¸ªå€™é€‰é¡¹å­—ç¬¦ä¸²ã€‚å¦‚æœä¸ç¡®å®šï¼Œè¯·è¿”å›â€œ%@â€ã€‚"
    },
    "Candidates:": {"zh-Hans": "å€™é€‰é¡¹ï¼š"},
    "Content:": {"zh-Hans": "å†…å®¹ï¼š"},
    "Search Conversations": {"zh-Hans": "æœç´¢å¯¹è¯"},
    "Search saved conversations by keyword, date, and whether they include images.": {
        "zh-Hans": "æŒ‰å…³é”®è¯ã€æ—¥æœŸä»¥åŠæ˜¯å¦åŒ…å«å›¾ç‰‡æœç´¢å·²ä¿å­˜çš„å¯¹è¯ã€‚"
    },
    "Keyword": {"zh-Hans": "å…³é”®è¯"},
    "Date": {"zh-Hans": "æ—¥æœŸ"},
    "Include Images": {"zh-Hans": "åŒ…å«å›¾ç‰‡"},
    "Search conversations": {"zh-Hans": "æœç´¢å¯¹è¯"},
    "Keyword: %@": {"zh-Hans": "å…³é”®è¯ï¼š%@"},
    "On date: %@": {"zh-Hans": "æ—¥æœŸï¼š%@"},
    "Only conversations with images": {"zh-Hans": "ä»…åŒ…å«å›¾ç‰‡çš„å¯¹è¯"},
    "No conversations found.": {"zh-Hans": "æœªæ‰¾åˆ°å¯¹è¯ã€‚"},
    "%d conversation(s) matched your criteria.": {"zh-Hans": "ç¬¦åˆæ¡ä»¶çš„å¯¹è¯æ•°ï¼š%dã€‚"},
    "%@ â€¢ %@": {"zh-Hans": "%@ â€¢ %@"},
    "[%@] %@": {"zh-Hans": "[%@] %@"},
    "(No content)": {"zh-Hans": "ï¼ˆæ— å†…å®¹ï¼‰"},
    "Classify": {"zh-Hans": "åˆ†ç±»"},
    "Search Chats": {"zh-Hans": "æœç´¢èŠå¤©"},
    "At least one candidate is required.": {"zh-Hans": "è‡³å°‘éœ€è¦ä¸€ä¸ªå€™é€‰é¡¹ã€‚"},
    "Classify %@": {"zh-Hans": "åˆ†ç±» %@"},
}

def update_translations(file_path):
    """Update missing translations in the xcstrings file."""
    
    # Read the file
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ File not found: {file_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ JSON decode error in {file_path}: {e}")
        sys.exit(1)
    
    strings = data['strings']

    # Ensure new strings exist with provided translations
    for key, translations in NEW_STRINGS.items():
        entry = strings.setdefault(key, {})
        if entry.get('shouldTranslate') is False:
            entry.pop('shouldTranslate', None)

        locs = entry.setdefault('localizations', {})
        locs.setdefault('en', {
            'stringUnit': {
                'state': 'translated',
                'value': key,
            }
        })

        for language, value in translations.items():
            locs[language] = {
                'stringUnit': {
                    'state': 'translated',
                    'value': value,
                }
            }

    # Determine all languages present in the file (excluding ones marked shouldTranslate=false)
    languages: set[str] = set()
    for value in strings.values():
        locs = value.get('localizations', {})
        for lang in locs.keys():
            languages.add(lang)

    # Ensure English is always part of the language set
    languages.add('en')

    # Count changes
    added_count = 0
    fixed_count = 0
    filled_count = 0
    
    # Iterate through all strings
    for key, value in strings.items():
        # Skip strings marked as shouldTranslate=false
        if not value.get('shouldTranslate', True):
            continue
        
        # Ensure dictionary exists for modifications
        if 'localizations' not in value:
            value['localizations'] = {}

        locs = value['localizations']

        # Check if 'en' localization is missing
        if 'en' not in locs:
            locs['en'] = {
                'stringUnit': {
                    'state': 'translated',
                    'value': key
                }
            }
            added_count += 1

        # Ensure English localization is properly marked
        en_loc = locs['en']
        en_string_unit = en_loc.setdefault('stringUnit', {})
        if en_string_unit.get('state') == 'new':
            if not en_string_unit.get('value', '').strip():
                en_string_unit['value'] = key
            en_string_unit['state'] = 'translated'
            fixed_count += 1
        english_value = en_string_unit.get('value', key)

        # Fill missing localizations for other languages using English as fallback
        for language in languages:
            if language == 'en':
                continue

            string_unit = locs.get(language, {}).get('stringUnit')
            current_value = string_unit.get('value').strip() if string_unit and string_unit.get('value') else ''
            current_state = string_unit.get('state') if string_unit else None

            if language not in locs or not current_value:
                locs[language] = {
                    'stringUnit': {
                        'state': 'translated',
                        'value': english_value
                    }
                }
                filled_count += 1
            elif current_state == 'new':
                locs[language]['stringUnit']['state'] = 'translated'
                if not current_value:
                    locs[language]['stringUnit']['value'] = english_value
                filled_count += 1
    
    # Write the updated file
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… Successfully updated {file_path}")
        print(f"   - Added {added_count} missing English localizations")
        print(f"   - Fixed {fixed_count} 'new' state translations")
        print(f"   - Filled {filled_count} fallback localizations")
        return True
    except Exception as e:
        print(f"âŒ Error writing file: {e}")
        sys.exit(1)

if __name__ == '__main__':
    # Default path to the Localizable.xcstrings file
    default_file_path = os.path.join(
        os.path.dirname(__file__), 
        '..', '..', 
        'FlowDown', 
        'Resources', 
        'Localizable.xcstrings'
    )
    
    # Allow overriding the file path via command line argument
    file_path = sys.argv[1] if len(sys.argv) > 1 else default_file_path
    
    print(f"ğŸ“ Updating translations in: {file_path}")
    update_translations(file_path)
